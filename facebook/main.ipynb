{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trand\\AppData\\Local\\Temp\\ipykernel_3620\\2925264960.py:121: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(chrome_driver_path, options=options)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "### Các bước chuẩn bị\n",
    "# Function to read proxy config from file\n",
    "def read_proxy_config(file_path):\n",
    "    config = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split('=')\n",
    "            config[key] = value\n",
    "    return config\n",
    "\n",
    "# Read proxy settings from text file\n",
    "proxy_config = read_proxy_config(r'C:\\working\\job_rcm\\job_rcm_code\\config.txt')\n",
    "PROXY_HOST = proxy_config['PROXY_HOST']\n",
    "PROXY_PORT = proxy_config['PROXY_PORT']\n",
    "PROXY_USER = proxy_config['PROXY_USER']\n",
    "PROXY_PASS = proxy_config['PROXY_PASS']\n",
    "\n",
    "# ChromeDriver path\n",
    "chrome_driver_path = 'C:/Users/trand/chromedriver.exe'\n",
    "\n",
    "# Manifest for proxy\n",
    "manifest_json = \"\"\"\n",
    "{\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"manifest_version\": 2,\n",
    "    \"name\": \"Chrome Proxy\",\n",
    "    \"permissions\": [\n",
    "        \"proxy\",\n",
    "        \"tabs\",\n",
    "        \"unlimitedStorage\",\n",
    "        \"storage\",\n",
    "        \"<all_urls>\",\n",
    "        \"webRequest\",\n",
    "        \"webRequestBlocking\"\n",
    "    ],\n",
    "    \"background\": {\n",
    "        \"scripts\": [\"background.js\"]\n",
    "    },\n",
    "    \"minimum_chrome_version\":\"22.0.0\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Background script for proxy\n",
    "background_js = \"\"\"\n",
    "var config = {\n",
    "        mode: \"fixed_servers\",\n",
    "        rules: {\n",
    "        singleProxy: {\n",
    "            scheme: \"http\",\n",
    "            host: \"%s\",\n",
    "            port: parseInt(%s)\n",
    "        },\n",
    "        bypassList: [\"localhost\"]\n",
    "        }\n",
    "    };\n",
    "\n",
    "chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});\n",
    "\n",
    "function callbackFn(details) {\n",
    "    return {\n",
    "        authCredentials: {\n",
    "            username: \"%s\",\n",
    "            password: \"%s\"\n",
    "        }\n",
    "    };\n",
    "}\n",
    "\n",
    "chrome.webRequest.onAuthRequired.addListener(\n",
    "            callbackFn,\n",
    "            {urls: [\"<all_urls>\"]},\n",
    "            ['blocking']\n",
    ");\n",
    "\"\"\" % (PROXY_HOST, PROXY_PORT, PROXY_USER, PROXY_PASS)\n",
    "\n",
    "# Set up Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "prefs = {\"credentials_enable_service\": False,\n",
    "     \"profile.password_manager_enabled\": False}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Randomize User-Agent\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "]\n",
    "options.add_argument(f\"user-agent={random.choice(user_agents)}\")\n",
    "\t\n",
    "# Set up proxy\n",
    "pluginfile = 'proxy_auth_plugin.zip'\n",
    "with zipfile.ZipFile(pluginfile, 'w') as zp:\n",
    "    zp.writestr(\"manifest.json\", manifest_json)\n",
    "    zp.writestr(\"background.js\", background_js)\n",
    "options.add_extension(pluginfile)\n",
    "\n",
    "\n",
    "# Launch browser\n",
    "driver = webdriver.Chrome(chrome_driver_path, options=options)\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# Login \n",
    "time.sleep(4)\n",
    "email = driver.find_element(By.NAME, \"email\")\n",
    "password = driver.find_element(By.NAME, \"pass\")\n",
    "email.send_keys(\"nguyenanhnguyen111666@gmail.com\")\n",
    "password.send_keys(\"tranduyluan11062003\")\n",
    "\n",
    "button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "# Open a new tab\n",
    "driver.execute_script(\"window.open('https://www.facebook.com/groups/datajobvn/');\")\n",
    "driver.switch_to.window(driver.window_handles[-1])\n",
    "# Scale screen to 60% for the new tab as well\n",
    "driver.execute_script(\"document.body.style.zoom='60%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up directories\n",
    "image_save_dir = r'C:\\working\\job_rcm\\data\\facebook\\post_image'\n",
    "csv_file_path = r'C:\\working\\job_rcm\\data\\facebook\\post.csv'\n",
    "\n",
    "if not os.path.exists(image_save_dir):\n",
    "    os.makedirs(image_save_dir)\n",
    "\n",
    "# Initialize lists for data\n",
    "post_href = []\n",
    "hr_id = []  # Fill with \"chưa lấy\"\n",
    "group_id = []  # Fill with \"group 1\"\n",
    "scrape_date = []\n",
    "author_name = []\n",
    "content = []\n",
    "image = []\n",
    "post_date = []\n",
    "\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "while True:\n",
    "    # Find the feed element\n",
    "    feed_element = driver.find_element(By.XPATH, '//*[@role=\"feed\"]')\n",
    "    post_elements = driver.find_elements(By.XPATH, \"//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\")\n",
    "    \n",
    "    for index, element in enumerate(post_elements):\n",
    "\t\t\t\t\t# Scroll to the current element\n",
    "\t\t\t\t\tactions.move_to_element(element).perform()\n",
    "\t\t\t\t\ttime.sleep(5)  # Wait for more posts to load\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Get post href\n",
    "\t\t\t\t\tpost_href_element = element.find_element(By.XPATH, \".//a[@role='link']\")\n",
    "\t\t\t\t\tpost_href_value = post_href_element.get_attribute('href')\n",
    "\n",
    "\t\t\t\t\t# Skip if post_href is already in the list\n",
    "\t\t\t\t\tif post_href_value in post_href:\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Get author name\n",
    "\t\t\t\t\tauthor_name_element = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\tauthor_name_value = author_name_element[0].text.strip()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tactions.move_to_element(author_name_element[0]).key_down(Keys.CONTROL).click(author_name_element[0]).key_up(Keys.CONTROL).perform()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Chờ cho tab mới mở ra\n",
    "\t\t\t\t\ttime.sleep(3)\n",
    "\t\t\t\t\t# Chuyển sang tab mới\n",
    "\t\t\t\t\t# Lấy danh sách các tab hiện tại\n",
    "\t\t\t\t\tcurrent_tabs = driver.window_handles\n",
    "\n",
    "\t\t\t\t\t# Kiểm tra nếu có ít nhất 2 tab\n",
    "\t\t\t\t\tif len(current_tabs) > 2:\n",
    "\t\t\t\t\t\t\t# Chuyển sang tab mới (tab cuối cùng trong danh sách tab)\n",
    "\t\t\t\t\t\t\tdriver.switch_to.window(current_tabs[-1])\n",
    "\n",
    "\t\t\t\t\t\t\t# Tìm href của hr và điền vào hr_id\n",
    "\t\t\t\t\t\t\tprofile_elements = driver.find_elements(By.CSS_SELECTOR, 'a.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx')\n",
    "\t\t\t\t\t\t\thr_id_value = profile_elements[6].get_attribute('href')\n",
    "\n",
    "\t\t\t\t\t\t\ttime.sleep(2)\n",
    "\n",
    "\t\t\t\t\t\t\t# Đóng tab mới nếu tồn tại tab thứ 2\n",
    "\t\t\t\t\t\t\tdriver.close()\n",
    "\n",
    "\t\t\t\t\t\t\t# Chuyển về tab chính (tab đầu tiên trong danh sách tab)\n",
    "\t\t\t\t\t\t\tdriver.switch_to.window(current_tabs[1])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tprint(\"Chỉ có một tab, không có tab nào để đóng.\")\n",
    "\n",
    "\t\t\t\t\ttime.sleep(5)\n",
    "\n",
    "\t\t\t\t\tgroup_id_value = \"group 1\"\n",
    "\t\t\t\t\tscrape_date_value = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t####### Get post date with try-except block\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\thover_elements = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\t\t\ttime.sleep(5)\n",
    "\t\t\t\t\t\t\tactions.move_to_element(hover_elements[1]).perform()\n",
    "\t\t\t\t\t\t\ttime.sleep(5)\n",
    "\t\t\t\t\t\t\t# date_element = driver.find_element(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.xlh3980.xvmahel.x1n0sxbx.x1nxh6w3.x1sibtaa.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\t\t\tdate_element = driver.find_element(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1nxh6w3.x1sibtaa.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\t\t\tpost_date_value = date_element.text\n",
    "\t\t\t\t\t\t\tprint(post_date_value)\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tprint(f\"Không lấy ngày đăng bài ở {index+1}\")\n",
    "\t\t\t\t\t\t\tpost_date_value = None\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t####### Check for \"see more\" button and click it if present\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\tsee_more_button = element.find_element(By.XPATH, \".//div[contains(@class, 'x1i10hfl') and contains(@role, 'button') and contains(text(), 'Xem thêm')]\")\n",
    "\t\t\t\t\t\t\tif see_more_button:\n",
    "\t\t\t\t\t\t\t\t\tsee_more_button.click()\n",
    "\t\t\t\t\t\t\t\t\ttime.sleep(1)  # Wait for content to expand\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tprint(\"Post không có nút xem thêm\")\n",
    "\n",
    "\t\t\t\t\t# Get the content\n",
    "\t\t\t\t\tcontent_element_1s = element.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "\n",
    "\t\t\t\t\tif len(content_element_1s) > 1:\n",
    "\t\t\t\t\t\t\tprint('1')\n",
    "\t\t\t\t\t\t\tcontent_value = content_element_1s[1].text\n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\ttime.sleep(1)\n",
    "\t\t\t\t\t\t\tprint('2')\n",
    "\t\t\t\t\t\t\tcontent_element_2s = element.find_elements(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u\")\n",
    "\n",
    "\t\t\t\t\t\t\tif content_element_2s:\n",
    "\t\t\t\t\t\t\t\t\tcontent_value = content_element_2s[0].text\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\tcontent_value = None\n",
    "\t\t\t\t\t\t\t\t\tprint(\"No content found for this post\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Get images\n",
    "\t\t\t\t\timage_elements = element.find_elements(By.TAG_NAME, 'img')\n",
    "\t\t\t\t\timage_urls = [img.get_attribute('src') for img in image_elements if int(img.get_attribute('width')) > 100 and int(img.get_attribute('height')) > 100]\n",
    "\t\t\t\t\tif image_urls:\n",
    "\t\t\t\t\t\t\tpost_image_dir = os.path.join(image_save_dir, f'post_{index+1}')\n",
    "\t\t\t\t\t\t\tif not os.path.exists(post_image_dir):\n",
    "\t\t\t\t\t\t\t\t\tos.makedirs(post_image_dir)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tfor i, image_url in enumerate(image_urls, start=1):\n",
    "\t\t\t\t\t\t\t\t\timage_path = os.path.join(post_image_dir, f'image_{i}.jpg')\n",
    "\t\t\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\t\t\t\timage_data = requests.get(image_url).content\n",
    "\t\t\t\t\t\t\t\t\t\t\twith open(image_path, 'wb') as img_file:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\timg_file.write(image_data)\n",
    "\t\t\t\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\t\t\t\tprint(f\"post {index+1} không có ảnh\")\n",
    "\t\t\t\t\t\t\timage_value = \", \".join(image_urls)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\timage_value = None\n",
    "\n",
    "\t\t\t\t\t# Append data to lists\n",
    "\t\t\t\t\tpost_href.append(post_href_value)\n",
    "\t\t\t\t\thr_id.append(hr_id_value)\n",
    "\t\t\t\t\tauthor_name.append(author_name_value)\n",
    "\t\t\t\t\tgroup_id.append(group_id_value)\n",
    "\t\t\t\t\tscrape_date.append(scrape_date_value)\n",
    "\t\t\t\t\tpost_date.append(post_date_value)\n",
    "\t\t\t\t\tcontent.append(content_value)\n",
    "\t\t\t\t\timage.append(image_value)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Create DataFrame and save to CSV after each iteration\n",
    "\t\t\t\t\tdf = pd.DataFrame({\n",
    "\t\t\t\t\t\t\t'post_href': post_href,\n",
    "\t\t\t\t\t\t\t'hr_id': hr_id,\n",
    "\t\t\t\t\t\t\t'author_name': author_name,\n",
    "\t\t\t\t\t\t\t'group_id': group_id,\n",
    "\t\t\t\t\t\t\t'scrape_date': scrape_date,\n",
    "\t\t\t\t\t\t\t'post_date': post_date,\n",
    "\t\t\t\t\t\t\t'content': content,\n",
    "\t\t\t\t\t\t\t'image': image\n",
    "\t\t\t\t\t})\n",
    "\t\t\t\t\tdf.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\t\t\t\t\tprint(f\"Lưu csv post {index+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoàn thiện code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trand\\AppData\\Local\\Temp\\ipykernel_17672\\1808688157.py:121: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(chrome_driver_path, options=options)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "### Các bước chuẩn bị\n",
    "# Function to read proxy config from file\n",
    "def read_proxy_config(file_path):\n",
    "    config = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split('=')\n",
    "            config[key] = value\n",
    "    return config\n",
    "\n",
    "# # Read proxy settings from text file\n",
    "# proxy_config = read_proxy_config(r'C:\\working\\job_rcm\\job_rcm_code\\config.txt')\n",
    "# PROXY_HOST = proxy_config['PROXY_HOST']\n",
    "# PROXY_PORT = proxy_config['PROXY_PORT']\n",
    "# PROXY_USER = proxy_config['PROXY_USER']\n",
    "# PROXY_PASS = proxy_config['PROXY_PASS']\n",
    "\n",
    "# # ChromeDriver path\n",
    "chrome_driver_path = 'C:/Users/trand/chromedriver.exe'\n",
    "\n",
    "# # Manifest for proxy\n",
    "# manifest_json = \"\"\"\n",
    "# {\n",
    "#     \"version\": \"1.0.0\",\n",
    "#     \"manifest_version\": 2,\n",
    "#     \"name\": \"Chrome Proxy\",\n",
    "#     \"permissions\": [\n",
    "#         \"proxy\",\n",
    "#         \"tabs\",\n",
    "#         \"unlimitedStorage\",\n",
    "#         \"storage\",\n",
    "#         \"<all_urls>\",\n",
    "#         \"webRequest\",\n",
    "#         \"webRequestBlocking\"\n",
    "#     ],\n",
    "#     \"background\": {\n",
    "#         \"scripts\": [\"background.js\"]\n",
    "#     },\n",
    "#     \"minimum_chrome_version\":\"22.0.0\"\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# # Background script for proxy\n",
    "# background_js = \"\"\"\n",
    "# var config = {\n",
    "#         mode: \"fixed_servers\",\n",
    "#         rules: {\n",
    "#         singleProxy: {\n",
    "#             scheme: \"http\",\n",
    "#             host: \"%s\",\n",
    "#             port: parseInt(%s)\n",
    "#         },\n",
    "#         bypassList: [\"localhost\"]\n",
    "#         }\n",
    "#     };\n",
    "\n",
    "# chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});\n",
    "\n",
    "# function callbackFn(details) {\n",
    "#     return {\n",
    "#         authCredentials: {\n",
    "#             username: \"%s\",\n",
    "#             password: \"%s\"\n",
    "#         }\n",
    "#     };\n",
    "# }\n",
    "\n",
    "# chrome.webRequest.onAuthRequired.addListener(\n",
    "#             callbackFn,\n",
    "#             {urls: [\"<all_urls>\"]},\n",
    "#             ['blocking']\n",
    "# );\n",
    "# \"\"\" % (PROXY_HOST, PROXY_PORT, PROXY_USER, PROXY_PASS)\n",
    "\n",
    "# Set up Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "prefs = {\"credentials_enable_service\": False,\n",
    "     \"profile.password_manager_enabled\": False}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Randomize User-Agent\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "]\n",
    "options.add_argument(f\"user-agent={random.choice(user_agents)}\")\n",
    "\t\n",
    "# # Set up proxy\n",
    "# pluginfile = 'proxy_auth_plugin.zip'\n",
    "# with zipfile.ZipFile(pluginfile, 'w') as zp:\n",
    "#     zp.writestr(\"manifest.json\", manifest_json)\n",
    "#     zp.writestr(\"background.js\", background_js)\n",
    "# options.add_extension(pluginfile)\n",
    "\n",
    "\n",
    "# Launch browser\n",
    "driver = webdriver.Chrome(chrome_driver_path, options=options)\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# Login \n",
    "time.sleep(4)\n",
    "email = driver.find_element(By.NAME, \"email\")\n",
    "password = driver.find_element(By.NAME, \"pass\")\n",
    "email.send_keys(\"nguyenanhnguyen111666@gmail.com\")\n",
    "password.send_keys(\"tranduyluan11062003\")\n",
    "\n",
    "button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "# # Open a new tab\n",
    "# driver.execute_script(\"window.open('https://www.facebook.com/groups/160325109320298/');\") \n",
    "# ### cần làm nhiều group trong \n",
    "\n",
    "# driver.switch_to.window(driver.window_handles[-1])\n",
    "# # Scale screen to 60% for the new tab as well\n",
    "# driver.execute_script(\"document.body.style.zoom='60%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logic code của lấy bài lấy fb \n",
    "\n",
    "- vòng lặp 1 : (lặp qua các group fb đã chọn, chỉ chuyển qua group mới khi đã lấy hết bài đăng của group này ) đăng tính có lẽ không cần vòng lặp \n",
    "- vòng lặp 2 nằm trong vòng lặp 1 (lấy hết bài đăng của các group, sửa logic,áp dụng index để không chạy từ trên xuống dưới)\n",
    "\n",
    "cấu trúc thư mục sẽ là \n",
    "data > facebook > năm (attempt cào) > tháng (attempt cào) > ngày (attempt cào) > giờ phút (attempt cào)\n",
    "giờ phút (attempt cào) > part (tối đa 300 thư mục 1 part, tự động tạo part mới khi đủ 300 thư mục) > 1 detail \n",
    "\n",
    "mỗi detail = một facebook post/tus\n",
    "tên detail : [thời gian cào] _ [FbGroupName] _ [PostHref] \n",
    "\n",
    "1 thư mục detail gồm : file data.json , thư mục ảnh (đặt tên mỗi file ảnh trong thư mục với stt, file html.txt = outerHTML của post đó ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json cấu trúc của mỗi post\n",
    "job i {\n",
    "\tpost_href = \n",
    "\tgroup_id =   # Fill with \"group_href\" = https://www.facebook.com/groups/160325109320298/\n",
    "\tscrape_date =  # còn áp dụng scrape_date cho việc tạo thư mục\n",
    "\tauthor_name = \n",
    "\tcontent = \n",
    "\timage_folder_path = \n",
    "\tpost_date = \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thứ Tư, 23 Tháng 10, 2024 lúc 18:00\n",
      "2\n",
      "Lưu csv post 1\n",
      "Không lấy ngày đăng bài ở 2\n",
      "1\n",
      "Lưu csv post 2\n",
      "Thứ Hai, 21 Tháng 10, 2024 lúc 18:05\n",
      "2\n",
      "Lưu csv post 3\n",
      "Thứ Hai, 21 Tháng 10, 2024 lúc 21:28\n",
      "1\n",
      "Lưu csv post 4\n",
      "Thứ Hai, 21 Tháng 10, 2024 lúc 21:19\n",
      "Post không có nút xem thêm\n",
      "1\n",
      "Lưu csv post 5\n",
      "Thứ Ba, 22 Tháng 10, 2024 lúc 15:25\n",
      "Post không có nút xem thêm\n",
      "1\n",
      "Lưu csv post 6\n",
      "Thứ Ba, 22 Tháng 10, 2024 lúc 15:24\n",
      "2\n",
      "Lưu csv post 7\n",
      "Thứ Ba, 22 Tháng 10, 2024 lúc 10:50\n",
      "2\n",
      "Lưu csv post 8\n",
      "Thứ Ba, 22 Tháng 10, 2024 lúc 10:49\n",
      "2\n",
      "Lưu csv post 9\n",
      "Thứ Hai, 21 Tháng 10, 2024 lúc 22:10\n",
      "1\n",
      "Lưu csv post 10\n",
      "Thứ Hai, 21 Tháng 10, 2024 lúc 23:56\n",
      "2\n",
      "Lưu csv post 11\n",
      "Thứ Hai, 21 Tháng 10, 2024 lúc 16:21\n",
      "Post không có nút xem thêm\n",
      "2\n",
      "Lưu csv post 12\n",
      "Chủ Nhật, 20 Tháng 10, 2024 lúc 01:06\n",
      "1\n",
      "Lưu csv post 13\n",
      "Thứ bảy, 19 Tháng 10, 2024 lúc 20:45\n",
      "2\n",
      "Lưu csv post 14\n",
      "Thứ Sáu, 18 Tháng 10, 2024 lúc 11:54\n",
      "2\n",
      "Lưu csv post 15\n",
      "Thứ Sáu, 11 Tháng 10, 2024 lúc 10:00\n",
      "2\n",
      "Lưu csv post 16\n",
      "Thứ Sáu, 18 Tháng 10, 2024 lúc 11:15\n",
      "1\n",
      "Lưu csv post 17\n",
      "Thứ Tư, 16 Tháng 10, 2024 lúc 23:52\n",
      "2\n",
      "Lưu csv post 18\n",
      "Thứ Tư, 16 Tháng 10, 2024 lúc 17:21\n",
      "Post không có nút xem thêm\n",
      "2\n",
      "Lưu csv post 19\n",
      "Không lấy ngày đăng bài ở 20\n",
      "Post không có nút xem thêm\n",
      "1\n",
      "Lưu csv post 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m author_name_element \u001b[38;5;241m=\u001b[39m element\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m author_name_value \u001b[38;5;241m=\u001b[39m author_name_element[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 43\u001b[0m \u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_to_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_name_element\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_down\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTROL\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_name_element\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTROL\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Chờ cho tab mới mở ra\u001b[39;00m\n\u001b[0;32m     46\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\selenium\\webdriver\\common\\action_chains.py:75\u001b[0m, in \u001b[0;36mActionChains.perform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    Performs all stored actions.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw3c_actions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\selenium\\webdriver\\common\\actions\\action_builder.py:77\u001b[0m, in \u001b[0;36mActionBuilder.perform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m         enc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(encoded)\n\u001b[0;32m     76\u001b[0m         device\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW3C_ACTIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:422\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    419\u001b[0m         params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m    421\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_value(params)\n\u001b[1;32m--> 422\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:421\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    419\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[0;32m    420\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:443\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    440\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 443\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\urllib3\\request.py:81\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     78\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\urllib3\\request.py:173\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    170\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[0;32m    171\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\working\\job_rcm\\job_rcm_code\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up directories\n",
    "image_save_dir = r'C:\\working\\job_rcm\\data\\facebook\\try\\post_image'\n",
    "csv_file_path = r'C:\\working\\job_rcm\\data\\facebook\\try\\post.csv'\n",
    "\n",
    "if not os.path.exists(image_save_dir):\n",
    "    os.makedirs(image_save_dir)\n",
    "\t\n",
    "\n",
    "# Initialize lists for data\n",
    "post_href = []\n",
    "hr_id = []  # Fill with \"chưa lấy\"\n",
    "group_id = []  # Fill with \"group 1\"\n",
    "scrape_date = []\n",
    "author_name = []\n",
    "content = []\n",
    "image = []\n",
    "post_date = []\n",
    "\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "while True:\n",
    "    # Find the feed element\n",
    "    feed_element = driver.find_element(By.XPATH, '//*[@role=\"feed\"]')\n",
    "    post_elements = driver.find_elements(By.XPATH, \"//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\")\n",
    "    \n",
    "    for index, element in enumerate(post_elements):\n",
    "\t\t\t\t\t# Scroll to the current element\n",
    "\t\t\t\t\tactions.move_to_element(element).perform()\n",
    "\t\t\t\t\ttime.sleep(5)  # Wait for more posts to load\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Get post href\n",
    "\t\t\t\t\tpost_href_element = element.find_element(By.XPATH, \".//a[@role='link']\")\n",
    "\t\t\t\t\tpost_href_value = post_href_element.get_attribute('href')\n",
    "\n",
    "\t\t\t\t\t# Skip if post_href is already in the list\n",
    "\t\t\t\t\tif post_href_value in post_href:\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Get author name\n",
    "\t\t\t\t\tauthor_name_element = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\tauthor_name_value = author_name_element[0].text.strip()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tactions.move_to_element(author_name_element[0]).key_down(Keys.CONTROL).click(author_name_element[0]).key_up(Keys.CONTROL).perform()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Chờ cho tab mới mở ra\n",
    "\t\t\t\t\ttime.sleep(3)\n",
    "\t\t\t\t\t# Chuyển sang tab mới\n",
    "\t\t\t\t\t# Lấy danh sách các tab hiện tại\n",
    "\t\t\t\t\tcurrent_tabs = driver.window_handles\n",
    "\n",
    "\t\t\t\t\t# Kiểm tra nếu có ít nhất 2 tab\n",
    "\t\t\t\t\tif len(current_tabs) > 2:\n",
    "\t\t\t\t\t\t\t# Chuyển sang tab mới (tab cuối cùng trong danh sách tab)\n",
    "\t\t\t\t\t\t\tdriver.switch_to.window(current_tabs[-1])\n",
    "\n",
    "\t\t\t\t\t\t\t# Tìm href của hr và điền vào hr_id\n",
    "\t\t\t\t\t\t\tprofile_elements = driver.find_elements(By.CSS_SELECTOR, 'a.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx')\n",
    "\t\t\t\t\t\t\thr_id_value = profile_elements[6].get_attribute('href')\n",
    "\n",
    "\t\t\t\t\t\t\ttime.sleep(2)\n",
    "\n",
    "\t\t\t\t\t\t\t# Đóng tab mới nếu tồn tại tab thứ 2\n",
    "\t\t\t\t\t\t\tdriver.close()\n",
    "\n",
    "\t\t\t\t\t\t\t# Chuyển về tab chính (tab đầu tiên trong danh sách tab)\n",
    "\t\t\t\t\t\t\tdriver.switch_to.window(current_tabs[1])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tprint(\"Chỉ có một tab, không có tab nào để đóng.\")\n",
    "\n",
    "\t\t\t\t\ttime.sleep(5)\n",
    "\n",
    "\t\t\t\t\tgroup_id_value = \"group 1\"\n",
    "\t\t\t\t\tscrape_date_value = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t####### Get post date with try-except block\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\thover_elements = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\t\t\ttime.sleep(5)\n",
    "\t\t\t\t\t\t\tactions.move_to_element(hover_elements[1]).perform()\n",
    "\t\t\t\t\t\t\ttime.sleep(5)\n",
    "\t\t\t\t\t\t\t# date_element = driver.find_element(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.xlh3980.xvmahel.x1n0sxbx.x1nxh6w3.x1sibtaa.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\t\t\tdate_element = driver.find_element(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1nxh6w3.x1sibtaa.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\t\t\tpost_date_value = date_element.text\n",
    "\t\t\t\t\t\t\tprint(post_date_value)\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tprint(f\"Không lấy ngày đăng bài ở {index+1}\")\n",
    "\t\t\t\t\t\t\tpost_date_value = None\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t####### Check for \"see more\" button and click it if present\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\tsee_more_button = element.find_element(By.XPATH, \".//div[contains(@class, 'x1i10hfl') and contains(@role, 'button') and contains(text(), 'Xem thêm')]\")\n",
    "\t\t\t\t\t\t\tif see_more_button:\n",
    "\t\t\t\t\t\t\t\t\tsee_more_button.click()\n",
    "\t\t\t\t\t\t\t\t\ttime.sleep(1)  # Wait for content to expand\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tprint(\"Post không có nút xem thêm\")\n",
    "\n",
    "\t\t\t\t\t# Get the content\n",
    "\t\t\t\t\tcontent_element_1s = element.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "\n",
    "\t\t\t\t\tif len(content_element_1s) > 1:\n",
    "\t\t\t\t\t\t\tprint('1')\n",
    "\t\t\t\t\t\t\tcontent_value = content_element_1s[1].text\n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\ttime.sleep(1)\n",
    "\t\t\t\t\t\t\tprint('2')\n",
    "\t\t\t\t\t\t\tcontent_element_2s = element.find_elements(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u\")\n",
    "\n",
    "\t\t\t\t\t\t\tif content_element_2s:\n",
    "\t\t\t\t\t\t\t\t\tcontent_value = content_element_2s[0].text\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\tcontent_value = None\n",
    "\t\t\t\t\t\t\t\t\tprint(\"No content found for this post\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Get images\n",
    "\t\t\t\t\timage_elements = element.find_elements(By.TAG_NAME, 'img')\n",
    "\t\t\t\t\timage_urls = [img.get_attribute('src') for img in image_elements if int(img.get_attribute('width')) > 100 and int(img.get_attribute('height')) > 100]\n",
    "\t\t\t\t\tif image_urls:\n",
    "\t\t\t\t\t\t\tpost_image_dir = os.path.join(image_save_dir, f'post_{index+1}')\n",
    "\t\t\t\t\t\t\tif not os.path.exists(post_image_dir):\n",
    "\t\t\t\t\t\t\t\t\tos.makedirs(post_image_dir)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tfor i, image_url in enumerate(image_urls, start=1):\n",
    "\t\t\t\t\t\t\t\t\timage_path = os.path.join(post_image_dir, f'image_{i}.jpg')\n",
    "\t\t\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\t\t\t\timage_data = requests.get(image_url).content\n",
    "\t\t\t\t\t\t\t\t\t\t\twith open(image_path, 'wb') as img_file:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\timg_file.write(image_data)\n",
    "\t\t\t\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\t\t\t\tprint(f\"post {index+1} không có ảnh\")\n",
    "\t\t\t\t\t\t\timage_value = \", \".join(image_urls)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\timage_value = None\n",
    "\n",
    "\t\t\t\t\t# Append data to lists\n",
    "\t\t\t\t\tpost_href.append(post_href_value)\n",
    "\t\t\t\t\thr_id.append(hr_id_value)\n",
    "\t\t\t\t\tauthor_name.append(author_name_value)\n",
    "\t\t\t\t\tgroup_id.append(group_id_value)\n",
    "\t\t\t\t\tscrape_date.append(scrape_date_value)\n",
    "\t\t\t\t\tpost_date.append(post_date_value)\n",
    "\t\t\t\t\tcontent.append(content_value)\n",
    "\t\t\t\t\timage.append(image_value)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Create DataFrame and save to CSV after each iteration\n",
    "\t\t\t\t\tdf = pd.DataFrame({\n",
    "\t\t\t\t\t\t\t'post_href': post_href,\n",
    "\t\t\t\t\t\t\t'hr_id': hr_id,\n",
    "\t\t\t\t\t\t\t'author_name': author_name,\n",
    "\t\t\t\t\t\t\t'group_id': group_id,\n",
    "\t\t\t\t\t\t\t'scrape_date': scrape_date,\n",
    "\t\t\t\t\t\t\t'post_date': post_date,\n",
    "\t\t\t\t\t\t\t'content': content,\n",
    "\t\t\t\t\t\t\t'image': image\n",
    "\t\t\t\t\t})\n",
    "\t\t\t\t\tdf.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\t\t\t\t\tprint(f\"Lưu csv post {index+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lấy cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies:\n",
      "{'domain': '.facebook.com', 'expiry': 1752639183, 'httpOnly': True, 'name': 'fr', 'path': '/', 'sameSite': 'None', 'secure': True, 'value': '0tRjTwCU2U4F8hWLU.AWcoGjxymr1qHnuUE_7SJ4aypY1pIsPrg3WdIg7SnhVz1qynmUU.BoAH_p..AAA.0.0.BoAIAC.AWeJkiLh5mrBf61GzdCORM0aXu8'}\n",
      "{'domain': '.facebook.com', 'expiry': 1776399182, 'httpOnly': True, 'name': 'xs', 'path': '/', 'sameSite': 'None', 'secure': True, 'value': '12%3AXIScaWSUJ_K5Wg%3A2%3A1744863231%3A-1%3A12159'}\n",
      "{'domain': '.facebook.com', 'expiry': 1776399182, 'httpOnly': False, 'name': 'c_user', 'path': '/', 'sameSite': 'None', 'secure': True, 'value': '100075350970914'}\n",
      "{'domain': '.facebook.com', 'expiry': 1745467986, 'httpOnly': False, 'name': 'dpr', 'path': '/', 'sameSite': 'None', 'secure': True, 'value': '1.5'}\n",
      "{'domain': '.facebook.com', 'httpOnly': False, 'name': 'presence', 'path': '/', 'sameSite': 'Lax', 'secure': True, 'value': 'C%7B%22t3%22%3A%5B%5D%2C%22utc3%22%3A1744863186555%2C%22v%22%3A1%7D'}\n",
      "{'domain': '.facebook.com', 'expiry': 1745467986, 'httpOnly': False, 'name': 'wd', 'path': '/', 'sameSite': 'Lax', 'secure': True, 'value': '1036x558'}\n",
      "{'domain': '.facebook.com', 'expiry': 1779423182, 'httpOnly': True, 'name': 'sb', 'path': '/', 'sameSite': 'None', 'secure': True, 'value': '6X8AaK0FsfdrpCv_IoiZ8BGO'}\n",
      "{'domain': '.facebook.com', 'expiry': 1779423158, 'httpOnly': True, 'name': 'datr', 'path': '/', 'sameSite': 'None', 'secure': True, 'value': '6X8AaFhhCNFfAJCzoXbvyBt6'}\n",
      "Cookies đã được lưu vào file facebook_cookies.json\n"
     ]
    }
   ],
   "source": [
    "# Lấy danh sách cookie từ trình duyệt\n",
    "cookies = driver.get_cookies()\n",
    "\n",
    "# In cookie ra màn hình\n",
    "print(\"Cookies:\")\n",
    "for cookie in cookies:\n",
    "    print(cookie)\n",
    "\n",
    "# Lưu cookie vào file JSON để sử dụng sau này\n",
    "with open(\"facebook_cookies.json\", \"w\") as file:\n",
    "    json.dump(cookies, file, indent=4)\n",
    "\n",
    "print(\"Cookies đã được lưu vào file facebook_cookies.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trand\\AppData\\Local\\Temp\\ipykernel_17116\\338910392.py:121: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(chrome_driver_path, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đăng nhập bằng cookie thành công!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import random\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Function to read proxy config from file\n",
    "def read_proxy_config(file_path):\n",
    "    config = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split('=')\n",
    "            config[key] = value\n",
    "    return config\n",
    "\n",
    "# Read proxy settings from text file\n",
    "proxy_config = read_proxy_config(r'C:\\working\\job_rcm\\job_rcm_code\\config.txt')\n",
    "PROXY_HOST = proxy_config['PROXY_HOST']\n",
    "PROXY_PORT = proxy_config['PROXY_PORT']\n",
    "PROXY_USER = proxy_config['PROXY_USER']\n",
    "PROXY_PASS = proxy_config['PROXY_PASS']\n",
    "\n",
    "# ChromeDriver path\n",
    "chrome_driver_path = 'C:/Users/trand/chromedriver.exe'\n",
    "\n",
    "# Proxy settings\n",
    "manifest_json = \"\"\"\n",
    "{\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"manifest_version\": 2,\n",
    "    \"name\": \"Chrome Proxy\",\n",
    "    \"permissions\": [\n",
    "        \"proxy\",\n",
    "        \"tabs\",\n",
    "        \"unlimitedStorage\",\n",
    "        \"storage\",\n",
    "        \"<all_urls>\",\n",
    "        \"webRequest\",\n",
    "        \"webRequestBlocking\"\n",
    "    ],\n",
    "    \"background\": {\n",
    "        \"scripts\": [\"background.js\"]\n",
    "    },\n",
    "    \"minimum_chrome_version\":\"22.0.0\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "background_js = \"\"\"\n",
    "var config = {\n",
    "        mode: \"fixed_servers\",\n",
    "        rules: {\n",
    "        singleProxy: {\n",
    "            scheme: \"http\",\n",
    "            host: \"%s\",\n",
    "            port: parseInt(%s)\n",
    "        },\n",
    "        bypassList: [\"localhost\"]\n",
    "        }\n",
    "    };\n",
    "\n",
    "chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});\n",
    "\n",
    "function callbackFn(details) {\n",
    "    return {\n",
    "        authCredentials: {\n",
    "            username: \"%s\",\n",
    "            password: \"%s\"\n",
    "        }\n",
    "    };\n",
    "}\n",
    "\n",
    "chrome.webRequest.onAuthRequired.addListener(\n",
    "            callbackFn,\n",
    "            {urls: [\"<all_urls>\"]},\n",
    "            ['blocking']\n",
    ");\n",
    "\"\"\" % (PROXY_HOST, PROXY_PORT, PROXY_USER, PROXY_PASS)\n",
    "\n",
    "# Set up Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "prefs = {\"credentials_enable_service\": False, \"profile.password_manager_enabled\": False}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Randomize User-Agent\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "]\n",
    "options.add_argument(f\"user-agent={random.choice(user_agents)}\")\n",
    "\n",
    "# Set up proxy\n",
    "pluginfile = 'proxy_auth_plugin.zip'\n",
    "with zipfile.ZipFile(pluginfile, 'w') as zp:\n",
    "    zp.writestr(\"manifest.json\", manifest_json)\n",
    "    zp.writestr(\"background.js\", background_js)\n",
    "options.add_extension(pluginfile)\n",
    "\n",
    "# Path lưu cookie\n",
    "cookie_file = r\"C:\\working\\job_rcm\\job_rcm_code\\job_scraping\\facebook\\facebook_cookies.json\"\n",
    "\n",
    "# Launch browser\n",
    "driver = webdriver.Chrome(chrome_driver_path, options=options)\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# Load cookies nếu có\n",
    "try:\n",
    "    with open(cookie_file, \"r\") as file:\n",
    "        cookies = json.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "    # Refresh để xem có đăng nhập thành công không\n",
    "    driver.refresh()\n",
    "    time.sleep(3)\n",
    "\n",
    "    if \"login\" not in driver.current_url:\n",
    "        print(\"Đăng nhập bằng cookie thành công!\")\n",
    "    else:\n",
    "        print(\"Cookie hết hạn, đăng nhập lại...\")\n",
    "        raise Exception(\"Cookie Expired\")\n",
    "except Exception as e:\n",
    "    print(\"Không thể đăng nhập bằng cookie:\", str(e))\n",
    "    \n",
    "    # Đăng nhập bằng tài khoản & mật khẩu\n",
    "    time.sleep(4)\n",
    "    email = driver.find_element(By.NAME, \"email\")\n",
    "    password = driver.find_element(By.NAME, \"pass\")\n",
    "    email.send_keys(\"nguyenanhnguyen111666@gmail.com\")\n",
    "    password.send_keys(\"tranduyluan11062003\")\n",
    "\n",
    "    button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "    # Chờ trang load hoàn tất\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Kiểm tra nếu đăng nhập thành công, lưu cookie mới\n",
    "    if \"login\" not in driver.current_url:\n",
    "        print(\"Đăng nhập thành công! Lưu cookie mới...\")\n",
    "        \n",
    "        # Lưu cookie vào file JSON\n",
    "        cookies = driver.get_cookies()\n",
    "        with open(cookie_file, \"w\") as file:\n",
    "            json.dump(cookies, file, indent=4)\n",
    "        \n",
    "        print(\"Cookies đã được lưu vào\", cookie_file)\n",
    "    else:\n",
    "        print(\"Đăng nhập thất bại! Vui lòng kiểm tra lại thông tin tài khoản.\")\n",
    "        \n",
    "time.sleep(5)\n",
    "# Open a new tab\n",
    "# driver.execute_script(\"window.open('https://www.facebook.com/groups/160325109320298/');\") \n",
    "driver.execute_script(\"window.open('https://www.facebook.com/groups/misdue/');\") \n",
    "\n",
    "### cần làm nhiều group trong \n",
    "\n",
    "driver.switch_to.window(driver.window_handles[-1])\n",
    "# Scale screen to 60% for the new tab as well\n",
    "driver.execute_script(\"document.body.style.zoom='60%'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Uyen Uyen\n",
      "Only one tab, no extra tab to close.\n",
      "2\n",
      "Ngọc Vy\n",
      "2\n",
      " TUYỂN SINH THẠC SĨ NGÀNH HỆ THỐNG THÔNG TIN QUẢN LÝ - ĐỢT 1/2025 \n",
      "Only one tab, no extra tab to close.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m see_more_button:\n\u001b[0;32m     83\u001b[0m \t\t\t\tsee_more_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m---> 84\u001b[0m \t\t\t\t\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     86\u001b[0m \t\t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msee more\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m button found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_save_dir = r'C:\\working\\job_rcm\\data\\facebook'\n",
    "csv_file_path = os.path.join(base_save_dir, 'post_hrefs.csv')\n",
    "\n",
    "def read_post_hrefs_from_csv():\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        return []\n",
    "    with open(csv_file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        return [row[0] for row in reader]\n",
    "\n",
    "# Function to save post href to the CSV file\n",
    "def save_post_href_to_csv(post_href):\n",
    "    with open(csv_file_path, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([post_href])\n",
    "\n",
    "group_href = \"https://www.facebook.com/groups/3272112773116278/\"\n",
    "\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "\n",
    "post_href_list = read_post_hrefs_from_csv()\n",
    "\n",
    "# Scraping loop\n",
    "while True:\n",
    "\t\t\tfeed_element = driver.find_element(By.XPATH, '//*[@role=\"feed\"]')\n",
    "\t\t\tpost_elements = driver.find_elements(By.XPATH, \"//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\")\n",
    "\t\t\t# scrape_time = time.localtime()  # Record the scrape time\n",
    "\n",
    "\t\t\tfor element in post_elements:\n",
    "\t\t\t\t\t\tscrape_time = time.localtime()  # Record the scrape time\n",
    "\t\t\t\t\t\tactions.move_to_element(element).perform()\n",
    "\t\t\t\t\t\ttime.sleep(1)  \n",
    "\n",
    "\t\t\t\t\t\t# Get post href\n",
    "\t\t\t\t\t\tpost_href_element = element.find_element(By.CSS_SELECTOR, \n",
    "\t\t\t\t\t\t\t\t\"div.html-div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1q0g3np a\")\n",
    "\t\t\t\t\t\tpost_href_value = post_href_element.get_attribute('href')\n",
    "\t\t\t\t\t\tif post_href_value in post_href_list:\n",
    "\t\t\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t\t# Get author name\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tauthor_name_element = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\t\t\t\tauthor_name_value = author_name_element[0].text.strip()\n",
    "\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tauthor_name_value = None\n",
    "\t\t\t\t\t\t# Switch tabs and get HR ID\n",
    "\t\t\t\t\t\tactions.move_to_element(author_name_element[0]).key_down(Keys.CONTROL).click(author_name_element[0]).key_up(Keys.CONTROL).perform()\n",
    "\t\t\t\t\t\ttime.sleep(2)\n",
    "\t\t\t\t\t\tcurrent_tabs = driver.window_handles\n",
    "\n",
    "\t\t\t\t\t\tif len(current_tabs) > 2:\n",
    "\t\t\t\t\t\t\t\tdriver.switch_to.window(current_tabs[-1])\n",
    "\t\t\t\t\t\t\t\ttry :\n",
    "\t\t\t\t\t\t\t\t\t\tprofile_elements = driver.find_elements(By.CSS_SELECTOR, 'a.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx')\n",
    "\t\t\t\t\t\t\t\t\t\thr_id_value = profile_elements[6].get_attribute('href')\n",
    "\t\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\t\t\thr_id_value = None\n",
    "\t\t\t\t\t\t\t\ttime.sleep(2)\n",
    "\t\t\t\t\t\t\t\tdriver.close()\n",
    "\t\t\t\t\t\t\t\tdriver.switch_to.window(current_tabs[1])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tprint(\"Only one tab, no extra tab to close.\")\n",
    "\n",
    "\t\t\t\t\t\t# Get post date\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\t\t\thover_elements = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\t\t\t\t\t\t# time.sleep(5)\n",
    "\t\t\t\t\t\t\t\t\t\tactions.move_to_element(hover_elements[1]).perform()\n",
    "\t\t\t\t\t\t\t\t\t\ttime.sleep(5)\n",
    "\t\t\t\t\t\t\t\t\t\thtml =driver.page_source\n",
    "\t\t\t\t\t\t\t\t\t\tdate_element = driver.find_element(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1nxh6w3.x1sibtaa.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\t\t\t\t\t\tpost_date_value = date_element.text\n",
    "\t\t\t\t\t\t\t\t\t\tprint(post_date_value)\n",
    "\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\t\t\tpost_date_value = None\n",
    "\n",
    "\t\t\t\t\t\t# Check for \"see more\" button and click if present\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tsee_more_button = element.find_element(By.XPATH, \".//div[contains(@class, 'x1i10hfl') and contains(@role, 'button') and contains(text(), 'Xem thêm')]\")\n",
    "\t\t\t\t\t\t\t\tif see_more_button:\n",
    "\t\t\t\t\t\t\t\t\t\tsee_more_button.click()\n",
    "\t\t\t\t\t\t\t\t\t\ttime.sleep(1)\n",
    "\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\t\tprint(\"No 'see more' button found\")\n",
    "\n",
    "\t\t\t\t\t\t# Get the content\n",
    "\t\t\t\t\t\ttry :\n",
    "\t\t\t\t\t\t\t\tcolor_content_elements = element.find_elements(By.CSS_SELECTOR, \"div.x6s0dn4.x78zum5.xdt5ytf.x5yr21d.xl56j7k.x10l6tqk.x17qophe.x13vifvy.xh8yej3\")\n",
    "\t\t\t\t\t\t\t\tcontent_element_1s = element.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "\n",
    "\t\t\t\t\t\t\t\tif color_content_elements:  # Kiểm tra nếu tìm thấy phần tử\n",
    "\t\t\t\t\t\t\t\t\t\tcolor_content = color_content_elements[0]\n",
    "\t\t\t\t\t\t\t\t\t\tcontent_element_1s = color_content.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\tif content_element_1s:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tprint('1')\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcontent_value = content_element_1s[0].text\n",
    "\t\t\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcontent_value = None\n",
    "\t\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\t\t\traise NoSuchElementException\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\texcept NoSuchElementException:\n",
    "\t\t\t\t\t\t\t\ttime.sleep(1)\n",
    "\t\t\t\t\t\t\t\tprint('2')\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tcontent_element_2s = element.find_elements(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\t\t\t\tcontent_value = content_element_2s[0].text if content_element_2s else None\n",
    "\t\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\t# Get images\n",
    "\t\t\t\t\t\timage_elements = element.find_elements(By.TAG_NAME, 'img')\n",
    "\t\t\t\t\t\timage_urls = [img.get_attribute('src') for img in image_elements if int(img.get_attribute('width')) > 300 and int(img.get_attribute('height')) > 300]\n",
    "\n",
    "\t\t\t\t\t\t# Save post data and images\n",
    "\t\t\t\t\t\tpost_data = {\n",
    "\t\t\t\t\t\t\t\t'post_href': post_href_value,\n",
    "\t\t\t\t\t\t\t\t'author_name': author_name_value,\n",
    "\t\t\t\t\t\t\t\t'hr_id': hr_id_value,\n",
    "\t\t\t\t\t\t\t\t'group_id': group_href,\n",
    "\t\t\t\t\t\t\t\t'scrape_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "\t\t\t\t\t\t\t\t'post_date': post_date_value,\n",
    "\t\t\t\t\t\t\t\t'content': content_value,\n",
    "\t\t\t\t\t\t\t\t'images': image_urls\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# save_post_data(post_data, element.get_attribute('outerHTML'), image_urls, scrape_time)\n",
    "\t\t\t\t\t\t# save_post_data(post_data,html, image_urls, scrape_time)\n",
    "\n",
    "\t\t\t\t\t\t# Append post_href_value to the CSV file\n",
    "\t\t\t\t\t\tsave_post_href_to_csv(post_href_value)\n",
    "\t\t\t\t\t\t# Also append to the list for any further processing in the same run\n",
    "\t\t\t\t\t\tpost_href_list.append(post_href_value)\n",
    "\n",
    "\t\t\t\t\t\t# Sleep before the next post to avoid rate limiting\n",
    "\t\t\t\t\t\ttime.sleep(1)\n",
    "\t\t\t\t\t\tprint(content_value)\n",
    "\n",
    "\t\t\t# Scroll down the page to load more posts\n",
    "\t\t\tdriver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\t\t\ttime.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "try:\n",
    "    color_content_elements = element.find_elements(By.CSS_SELECTOR, \"div.x6s0dn4.x78zum5.xdt5ytf.x5yr21d.xl56j7k.x10l6tqk.x17qophe.x13vifvy.xh8yej3\")\n",
    "    \n",
    "    if color_content_elements:  # Kiểm tra nếu tìm thấy phần tử\n",
    "        color_content = color_content_elements[0]\n",
    "        content_element_1s = color_content.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "        \n",
    "        if content_element_1s:\n",
    "            print('1')\n",
    "            content_value = content_element_1s[0].text\n",
    "        else:\n",
    "            content_value = None\n",
    "    else:\n",
    "        raise NoSuchElementException  # Giả lập lỗi để vào except\n",
    "\n",
    "except NoSuchElementException:\n",
    "    time.sleep(1)\n",
    "    print('2')\n",
    "    \n",
    "    content_element_2s = element.find_elements(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u\")\n",
    "    content_value = content_element_2s[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_element = driver.find_element(By.XPATH, '//*[@role=\"feed\"]')\n",
    "post_elements = driver.find_elements(By.XPATH, \"//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\")\n",
    "element = post_elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"d0a8ddf117a7f26a0fe6aa701b281783\", element=\"f.B34DEF2FF611C20C081931B0A3C61D1D.d.FD64B9E629CC43C3756B3DDCCB7BDA78.e.292\")>]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.facebook.com/groups/160325109320298/posts/543122581040547/?__cft__[0]=AZVwfB6aGg-pI00TT6Sk_jTqGZea4Mn_ipeUeo0zhq9fQuUaLU2_DMGi3wDuAB140fWup56PUixAL-8cLBRX57hYHLAfup07vhA1I5NhyTyptMbNfFIQGVwMeKGVyZikifp9AYRZT08b1M1xr1OXYIgnjhy5496EeV8q8cvl0z-nGPmIK1caF0nozuM9zU5WelcjdF9S1HLNKtdVVvK7gqoJ&__tn__=%2CO%2CP-R'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_href_element = element.find_element(By.CSS_SELECTOR, \n",
    "\t\t\t\t\t\t\t\t\"div.html-div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1q0g3np a\"\n",
    "\t\t\t\t\t\t)\n",
    "post_href_value = post_href_element.get_attribute('href')\n",
    "post_href_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nộp đơn ở đâu chưa người đẹp?\\nChưa thì nộp ở TC Data đi.\\nTC Data tuyển dụng: Power Platform Developer… Xem thêm'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Lọc danh sách URL từ các ảnh có kích thước lớn hơn 300x300\n",
    "img_urls = [\n",
    "    img.get_attribute('src') for img in image_elements\n",
    "    if int(img.get_attribute('width')) > 300 and int(img.get_attribute('height')) > 300\n",
    "]\n",
    "\n",
    "for img_url in img_urls:\n",
    "    try:\n",
    "        response = requests.get(img_url)  # Tải từng ảnh một\n",
    "        response.raise_for_status()  # Kiểm tra nếu có lỗi HTTP\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        display(image)  # Hiển thị ảnh\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import json\n",
    "import csv\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cài đặt path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromeDriver path\n",
    "chrome_driver_path = 'C:/Users/trand/chromedriver.exe'\n",
    "\n",
    "# Setup directory paths\n",
    "base_save_dir = r'C:\\working\\job_rcm\\data\\facebook'\n",
    "image_save_dir = os.path.join(base_save_dir, 'post_image')\n",
    "csv_file_path = os.path.join(base_save_dir, 'post_hrefs.csv')\n",
    "group_href_csv_path = os.path.join(base_save_dir, 'group_href.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cài đặt và viết các hàm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc danh sách group từ CSV\n",
    "def read_group_hrefs():\n",
    "    groups = []\n",
    "    if os.path.exists(group_href_csv_path):\n",
    "        with open(group_href_csv_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Bỏ qua tiêu đề\n",
    "            for row in reader:\n",
    "                if len(row) >= 1:\n",
    "                    groups.append([row[0], row[1] if len(row) > 1 else \"\"])\n",
    "    return groups\n",
    "\n",
    "def update_last_scraped(group_href):\n",
    "    groups = read_group_hrefs()\n",
    "    for group in groups:\n",
    "        if group[0] == group_href:\n",
    "            group[1] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(group_href_csv_path, 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"group_href\", \"last_scraped\"])\n",
    "        writer.writerows(groups)\n",
    "\n",
    "group_hrefs = read_group_hrefs()\n",
    "current_group_index = 0  # Biến lưu trạng thái group hiện tại\n",
    "\n",
    "def get_next_group_href():\n",
    "    global current_group_index\n",
    "    if current_group_index < len(group_hrefs):\n",
    "        group_href = group_hrefs[current_group_index][0]\n",
    "        current_group_index += 1\n",
    "        return group_href\n",
    "    return None\n",
    "\n",
    "# Create directory if it does not exist\n",
    "if not os.path.exists(image_save_dir):\n",
    "    os.makedirs(image_save_dir)\n",
    "\n",
    "# Function to read post hrefs from the CSV file\n",
    "def read_post_hrefs_from_csv():\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        return []\n",
    "    with open(csv_file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        return [row[0] for row in reader]\n",
    "\n",
    "# Function to save post href to the CSV file\n",
    "def save_post_href_to_csv(post_href):\n",
    "    with open(csv_file_path, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([post_href])\n",
    "\n",
    "# Hàm chuyển đổi struct_time thành đối tượng datetime\n",
    "def convert_scrape_time(scrape_time):\n",
    "    return datetime.fromtimestamp(time.mktime(scrape_time))\n",
    "\n",
    "# Hàm tạo thư mục theo thời gian scrape\n",
    "def create_directory_structure(scrape_time):\n",
    "    scrape_time = convert_scrape_time(scrape_time)  # Chuyển struct_time thành datetime\n",
    "    year_dir = os.path.join(base_save_dir, scrape_time.strftime(\"%Y\"))\n",
    "    month_dir = os.path.join(year_dir, scrape_time.strftime(\"%m\"))\n",
    "    day_dir = os.path.join(month_dir, scrape_time.strftime(\"%d\"))\n",
    "    hour_dir = os.path.join(day_dir, scrape_time.strftime(\"%H\"))\n",
    "\n",
    "    for i in range(1, 100):  # Tối đa 100 phần\n",
    "        part_dir = os.path.join(hour_dir, f'part_{i}')\n",
    "        if not os.path.exists(part_dir):\n",
    "            os.makedirs(part_dir)\n",
    "            return part_dir\n",
    "\n",
    "        if len(os.listdir(part_dir)) < 300:  # Mỗi thư mục chứa tối đa 300 post\n",
    "            return part_dir\n",
    "\n",
    "# Hàm lưu dữ liệu post\n",
    "def save_post_data(post_data, post_html, images, scrape_time):\n",
    "    part_dir = create_directory_structure(scrape_time)\n",
    "\n",
    "    scrape_time_str = convert_scrape_time(scrape_time).strftime('%Y-%m-%d_%H%M%S')  # Thêm giây để đảm bảo tên duy nhất\n",
    "    detail_folder_name = f\"{scrape_time_str}_group_{post_data['group_id'].split('/')[-2]}\"\n",
    "    detail_folder = os.path.join(part_dir, detail_folder_name)\n",
    "    os.makedirs(detail_folder, exist_ok=True)\n",
    "\n",
    "    # Lưu file JSON\n",
    "    json_path = os.path.join(detail_folder, 'data.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(post_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Lưu file HTML\n",
    "    html_path = os.path.join(detail_folder, 'html.txt')\n",
    "    with open(html_path, 'w', encoding='utf-8') as html_file:\n",
    "        html_file.write(post_html)\n",
    "\n",
    "    # Lưu hình ảnh\n",
    "    image_folder = os.path.join(detail_folder, 'images')\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    for idx, img_url in enumerate(images):\n",
    "        img_path = os.path.join(image_folder, f'image_{idx+1}.jpg')\n",
    "        try:\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                img_file.write(img_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image: {e}\")\n",
    "\n",
    "################# Set up Chrome options and driver #################\n",
    "\n",
    "def read_config(file_path):\n",
    "    config = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if '=' in line:\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                config[key] = value\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "# Read proxy settings from text file\n",
    "proxy_config = read_config(r'C:\\working\\job_rcm\\job_rcm_code\\config.txt')\n",
    "\n",
    "PROXY_HOST = proxy_config.get('PROXY_HOST', '')\n",
    "PROXY_PORT = proxy_config.get('PROXY_PORT', '')\n",
    "PROXY_USER = proxy_config.get('PROXY_USER', '')\n",
    "PROXY_PASS = proxy_config.get('PROXY_PASS', '')\n",
    "\n",
    "# Proxy settings\n",
    "manifest_json = \"\"\"\n",
    "{\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"manifest_version\": 2,\n",
    "    \"name\": \"Chrome Proxy\",\n",
    "    \"permissions\": [\n",
    "        \"proxy\",\n",
    "        \"tabs\",\n",
    "        \"unlimitedStorage\",\n",
    "        \"storage\",\n",
    "        \"<all_urls>\",\n",
    "        \"webRequest\",\n",
    "        \"webRequestBlocking\"\n",
    "    ],\n",
    "    \"background\": {\n",
    "        \"scripts\": [\"background.js\"]\n",
    "    },\n",
    "    \"minimum_chrome_version\":\"22.0.0\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "background_js = \"\"\"\n",
    "var config = {\n",
    "        mode: \"fixed_servers\",\n",
    "        rules: {\n",
    "        singleProxy: {\n",
    "            scheme: \"http\",\n",
    "            host: \"%s\",\n",
    "            port: parseInt(%s)\n",
    "        },\n",
    "        bypassList: [\"localhost\"]\n",
    "        }\n",
    "    };\n",
    "\n",
    "chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});\n",
    "\n",
    "function callbackFn(details) {\n",
    "    return {\n",
    "        authCredentials: {\n",
    "            username: \"%s\",\n",
    "            password: \"%s\"\n",
    "        }\n",
    "    };\n",
    "}\n",
    "\n",
    "chrome.webRequest.onAuthRequired.addListener(\n",
    "            callbackFn,\n",
    "            {urls: [\"<all_urls>\"]},\n",
    "            ['blocking']\n",
    ");\n",
    "\"\"\" % (PROXY_HOST, PROXY_PORT, PROXY_USER, PROXY_PASS)\n",
    "\n",
    "# Set up Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "prefs = {\"credentials_enable_service\": False,\n",
    "     \"profile.password_manager_enabled\": False}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Randomize User-Agent\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "]\n",
    "options.add_argument(f\"user-agent={random.choice(user_agents)}\")\n",
    "\t\n",
    "# Set up proxy\n",
    "pluginfile = 'proxy_auth_plugin.zip'\n",
    "with zipfile.ZipFile(pluginfile, 'w') as zp:\n",
    "    zp.writestr(\"manifest.json\", manifest_json)\n",
    "    zp.writestr(\"background.js\", background_js)\n",
    "options.add_extension(pluginfile)\n",
    "\n",
    "# Path lưu cookie\n",
    "cookie_file = r\"C:\\working\\job_rcm\\job_rcm_code\\job_scraping\\facebook\\facebook_cookies.json\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_post(driver, element, actions, post_href_list, group_href):\n",
    "\t\t\t\"\"\"Hàm để lấy thông tin bài viết từ một phần tử web\"\"\"\n",
    "\t\t\t\n",
    "\t\t\tscrape_time = time.localtime()  # Ghi lại thời gian scrape\n",
    "\t\t\tactions.move_to_element(element).perform()\n",
    "\t\t\ttime.sleep(1)  \n",
    "\n",
    "\t\t\t# Lấy post href\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tpost_href_element = element.find_element(By.CSS_SELECTOR, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"div.html-div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1q0g3np a\")\n",
    "\t\t\t\t\tpost_href_value = post_href_element.get_attribute('href')\n",
    "\t\t\t\t\tif post_href_value in post_href_list:\n",
    "\t\t\t\t\t\t\treturn None\n",
    "\t\t\texcept NoSuchElementException:\n",
    "\t\t\t\t\tpost_href_value = None\n",
    "\n",
    "\t\t\t# Lấy tên tác giả\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tauthor_name_element = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\tauthor_name_value = author_name_element[0].text.strip()\n",
    "\t\t\texcept:\n",
    "\t\t\t\t\tauthor_name_value = None\n",
    "\n",
    "\t\t\t# Chuyển tab và lấy HR ID\n",
    "\t\t\tactions.move_to_element(author_name_element[0]).key_down(Keys.CONTROL).click(author_name_element[0]).key_up(Keys.CONTROL).perform()\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\tcurrent_tabs = driver.window_handles\n",
    "\t\t\t\n",
    "\t\t\tif len(current_tabs) > 2:\n",
    "\t\t\t\t\tdriver.switch_to.window(current_tabs[-1])\n",
    "\t\t\t\t\ttry :\n",
    "\t\t\t\t\t\t\t\t\tprofile_elements = driver.find_elements(By.CSS_SELECTOR, 'a.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx')\n",
    "\t\t\t\t\t\t\t\t\thr_id_value = profile_elements[6].get_attribute('href')\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\t\thr_id_value = None\n",
    "\t\t\t\t\ttime.sleep(2)\n",
    "\t\t\t\t\tdriver.close()\n",
    "\t\t\t\t\tdriver.switch_to.window(current_tabs[1])\n",
    "\t\t\telse:\n",
    "\t\t\t\t\tprint(\"Only one tab, no extra tab to close.\")\n",
    "\n",
    "\t\t\t# Lấy ngày đăng bài\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\thover_elements = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "\t\t\t\t\tactions.move_to_element(hover_elements[1]).perform()\n",
    "\n",
    "\t\t\t\t\ttime.sleep(5)\n",
    "\n",
    "\t\t\t\t\tdate_element = driver.find_element(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1nxh6w3.x1sibtaa.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\tpost_date_value = date_element.text\n",
    "\t\t\t\t\tprint(post_date_value)\n",
    "\t\t\texcept:\n",
    "\t\t\t\t\tpost_date_value = None\n",
    "\n",
    "\t\t\t# Kiểm tra và bấm \"Xem thêm\" nếu có\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tsee_more_button = element.find_element(By.XPATH, \".//div[contains(@class, 'x1i10hfl') and contains(@role, 'button') and contains(text(), 'Xem thêm')]\")\n",
    "\t\t\t\t\tif see_more_button:\n",
    "\t\t\t\t\t\t\tsee_more_button.click()\n",
    "\t\t\t\t\t\t\ttime.sleep(1)\n",
    "\t\t\texcept:\n",
    "\t\t\t\t\tprint(\"No 'see more' button found\")\n",
    "\n",
    "\t\t\t# Lấy nội dung bài viết\n",
    "\t\t\tcontent_value = None\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tcolor_content_elements = element.find_elements(By.CSS_SELECTOR, \"div.x6s0dn4.x78zum5.xdt5ytf.x5yr21d.xl56j7k.x10l6tqk.x17qophe.x13vifvy.xh8yej3\")\n",
    "\n",
    "\t\t\t\t\tif color_content_elements:\n",
    "\t\t\t\t\t\t\tcolor_content = color_content_elements[0]\n",
    "\t\t\t\t\t\t\tcontent_element_1s = color_content.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "\n",
    "\t\t\t\t\t\t\tcontent_value = content_element_1s[0].text if content_element_1s else None\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\traise NoSuchElementException\n",
    "\t\t\t\t\t\n",
    "\t\t\texcept NoSuchElementException:\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\tcontent_element_2s = element.find_elements(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u\")\n",
    "\t\t\t\t\t\t\tcontent_value = content_element_2s[0].text if content_element_2s else None\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\tcontent_value = None\n",
    "\n",
    "\t\t\t# Lấy ảnh\n",
    "\t\t\timage_elements = element.find_elements(By.TAG_NAME, 'img')\n",
    "\t\t\timage_urls = [img.get_attribute('src') for img in image_elements if img.get_attribute('width') and img.get_attribute('height') and int(img.get_attribute('width')) > 300 and int(img.get_attribute('height')) > 300]\n",
    "\n",
    "\t\t\t# Lưu dữ liệu bài viết\n",
    "\t\t\tpost_data = {\n",
    "\t\t\t\t\t'post_href': post_href_value,\n",
    "\t\t\t\t\t'author_name': author_name_value,\n",
    "\t\t\t\t\t'hr_id': hr_id_value,\n",
    "\t\t\t\t\t'group_id': group_href,\n",
    "\t\t\t\t\t'scrape_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "\t\t\t\t\t'post_date': post_date_value,\n",
    "\t\t\t\t\t'content': content_value,\n",
    "\t\t\t\t\t'images': image_urls\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t# save_post_data(post_data,html, image_urls, scrape_time)\n",
    "\t\t\tsave_post_href_to_csv(post_href_value)\n",
    "\t\t\tpost_href_list.append(post_href_value)\n",
    "\n",
    "\t\t\treturn post_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### khởi chạy drive và đăng nhập"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trand\\AppData\\Local\\Temp\\ipykernel_18728\\93514452.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(chrome_driver_path, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đăng nhập bằng cookie thành công!\n"
     ]
    }
   ],
   "source": [
    "# Launch browser\n",
    "driver = webdriver.Chrome(chrome_driver_path, options=options)\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# Load cookies nếu có\t\n",
    "try:\n",
    "    with open(cookie_file, \"r\") as file:\n",
    "        cookies = json.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "\n",
    "    # Refresh để xem có đăng nhập thành công không\n",
    "    driver.refresh()\n",
    "    time.sleep(3)\n",
    "\n",
    "    if \"login\" not in driver.current_url:\n",
    "        print(\"Đăng nhập bằng cookie thành công!\")\n",
    "    else:\n",
    "        print(\"Cookie hết hạn, đăng nhập lại...\")\n",
    "        raise Exception(\"Cookie Expired\")\n",
    "except Exception as e:\n",
    "    print(\"Không thể đăng nhập bằng cookie:\", str(e))\n",
    "    \n",
    "    # Đăng nhập bằng tài khoản & mật khẩu\n",
    "    time.sleep(4)\n",
    "    email = driver.find_element(By.NAME, \"email\")\n",
    "    password = driver.find_element(By.NAME, \"pass\")\n",
    "    email.send_keys(\"nguyenanhnguyen111666@gmail.com\")\n",
    "    password.send_keys(\"tranduyluan11062003\")\n",
    "\n",
    "    button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "    # Chờ trang load hoàn tất\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Kiểm tra nếu đăng nhập thành công, lưu cookie mới\n",
    "    if \"login\" not in driver.current_url:\n",
    "        print(\"Đăng nhập thành công! Lưu cookie mới...\")\n",
    "        \n",
    "        # Lưu cookie vào file JSON\n",
    "        cookies = driver.get_cookies()\n",
    "        with open(cookie_file, \"w\") as file:\n",
    "            json.dump(cookies, file, indent=4)\n",
    "        \n",
    "        print(\"Cookies đã được lưu vào\", cookie_file)\n",
    "    else:\n",
    "        print(\"Đăng nhập thất bại! Vui lòng kiểm tra lại thông tin tài khoản.\")\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang truy cập nhóm: https://www.facebook.com/groups/160325109320298/\n",
      "{'post_href': 'https://www.facebook.com/groups/160325109320298/posts/543122581040547/?__cft__[0]=AZUR_ZqSBzitCqjLr6SMZMtmw4NwvQ7wmg-5aNUMx2-4prQTfU0lYdjpGveXKb5epA66uVLv8rVDbFRql7abtc0GrHaChggt4yB3h1w-N_JbLFRXn3TnCyVWbD3RLUT_iJgqpq9f1NWykuTZoPo1B2pe_kPMXFch43YwOV6DT-bNX8TK1BTW2wc76Hf1f8npG9yvDk-fR0v5M5TYZ5c1Sy-b&__tn__=%2CO%2CP-R', 'author_name': 'Phương Uyên', 'hr_id': 'https://www.facebook.com/profile.php?id=100003871125916', 'group_id': 'https://www.facebook.com/groups/160325109320298/', 'scrape_date': '2025-03-12 14:16:18', 'post_date': None, 'content': None, 'images': ['https://scontent-sin2-2.xx.fbcdn.net/v/t39.30808-6/337171979_749691466868916_2917163268654261696_n.jpg?_nc_cat=103&ccb=1-7&_nc_sid=aa7b47&_nc_ohc=zcYVkNdG0LcQ7kNvgHv-B4Y&_nc_oc=AdgDIuDXfXr54CivVCwJ_d2N_AYeWZYHIUcmXJTeaCcrNu_Eue4kKl0ZL8ndnDwQs1xa6cuNT1noBzBa6COkSme2&_nc_zt=23&_nc_ht=scontent-sin2-2.xx&_nc_gid=AACGMUQOe2R6qoCmrf37xjy&oh=00_AYFb3mzsJ0T3ln5pjV0HgHg4BjkzVY338o6W6hhA72aA4w&oe=67D6F374', 'https://scontent-sin6-2.xx.fbcdn.net/v/t39.30808-6/336912942_1539130349914391_2144364717688672534_n.jpg?stp=dst-jpg_p960x960_tt6&_nc_cat=109&ccb=1-7&_nc_sid=aa7b47&_nc_ohc=7a53TkZIpzQQ7kNvgET3vIr&_nc_oc=Adg0D3bh0-Mu5IKz8rLQxBS2i1lvrbXHmRajvIDTkFQ9B4jsbsj2bzud5GEd-HXlK5wbhLZxizLKAiBFmeN5yxu1&_nc_zt=23&_nc_ht=scontent-sin6-2.xx&_nc_gid=AACGMUQOe2R6qoCmrf37xjy&oh=00_AYGydb01FREx_SjE5invdLxEKAHOTkk5Ihmu4wjW45M83g&oe=67D6EEBB']}\n",
      "Đã thu thập 1 bài trong nhóm https://www.facebook.com/groups/160325109320298/\n",
      "Hoàn thành nhóm https://www.facebook.com/groups/160325109320298/, cập nhật last_scraped và chuyển sang nhóm tiếp theo...\n",
      "Đang truy cập nhóm: https://www.facebook.com/groups/vieclammarketing43/\n",
      "{'post_href': 'https://www.facebook.com/groups/vieclammarketing43/posts/1756043274967740/?__cft__[0]=AZUWe6Ugx87OgRzzHv1fcElxqgBhZUXda_olAulP49I0Y1QRpbMGOJGISiqf6AdQULlFl8l3EKiI9_ApBrr5E_30BSoXOEOMemuLdOFgrwZmq1nLUps2DEQRVfPSUfOfjfSglTbqmFzMHXfYqHQs6MV8VmzNLsL-1xA_jTMTBh-WD04ovTZAGnGqR_8t69z_GJw&__tn__=%2CO%2CP-R', 'author_name': 'Phương Tiên Nguyễn', 'hr_id': 'https://www.facebook.com/profile.php?id=100087845205629', 'group_id': 'https://www.facebook.com/groups/vieclammarketing43/', 'scrape_date': '2025-03-12 14:16:39', 'post_date': None, 'content': None, 'images': []}\n",
      "Đã thu thập 1 bài trong nhóm https://www.facebook.com/groups/vieclammarketing43/\n",
      "Hoàn thành nhóm https://www.facebook.com/groups/vieclammarketing43/, cập nhật last_scraped và chuyển sang nhóm tiếp theo...\n",
      "Không còn nhóm nào để thu thập, dừng chương trình.\n"
     ]
    }
   ],
   "source": [
    "# Duyệt qua các nhóm\n",
    "group_href = get_next_group_href()\n",
    "post_href_list = read_post_hrefs_from_csv()\n",
    "\n",
    "while group_href:\n",
    "    print(f\"Đang truy cập nhóm: {group_href}\")\n",
    "    driver.execute_script(f\"window.open('{group_href}');\")\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    driver.execute_script(\"document.body.style.zoom='60%'\")\n",
    "\n",
    "    post_count = 0  # Reset post count mỗi lần đổi nhóm\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    while post_count < 5:  # Số lượng bài cần thu thập trước khi đổi nhóm\n",
    "        try:\n",
    "            feed_element = driver.find_element(By.XPATH, '//*[@role=\"feed\"]')\n",
    "            post_elements = driver.find_elements(By.XPATH, \"//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\")\n",
    "            for element in post_elements:\n",
    "                \n",
    "                post_data = scrape_post(driver, element, actions, post_href_list, group_href)\n",
    "                print(post_data)\n",
    "                \n",
    "\t\t\t\t\t\t\t\t\n",
    "                time.sleep(1)\n",
    "                post_count += 1\n",
    "                print(f\"Đã thu thập {post_count} bài trong nhóm {group_href}\")\n",
    "                if post_count >= 5:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi thu thập bài viết: {e}\")\n",
    "        \n",
    "        # Cuộn xuống để tải thêm bài\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(f\"Hoàn thành nhóm {group_href}, cập nhật last_scraped và chuyển sang nhóm tiếp theo...\")\n",
    "    update_last_scraped(group_href)\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    \n",
    "    group_href = get_next_group_href()\n",
    "\n",
    "print(\"Không còn nhóm nào để thu thập, dừng chương trình.\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Open a new tab\n",
    "# driver.execute_script(\"window.open('https://www.facebook.com/groups/160325109320298/');\") \n",
    "\n",
    "# driver.execute_script(f\"window.open('{group_href}');\")\n",
    "\n",
    "# driver.switch_to.window(driver.window_handles[-1])\n",
    "# # Scale screen to 60% for the new tab as well\n",
    "# driver.execute_script(\"document.body.style.zoom='60%'\")\n",
    "\n",
    "group_href = get_next_group_href()\n",
    "\n",
    "#################################\n",
    "# Scraping data #################\n",
    "#################################\n",
    "\n",
    "actions = ActionChains(driver)\n",
    "post_href_list = read_post_hrefs_from_csv()\n",
    "\n",
    "post_count = 0 \n",
    "n = 1\n",
    "\n",
    "\n",
    "# Scraping loop\n",
    "while True:\n",
    "        feed_element = driver.find_element(By.XPATH, '//*[@role=\"feed\"]')\n",
    "        post_elements = driver.find_elements(By.XPATH, \"//div[@class='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z']\")\n",
    "        # scrape_time = time.localtime()  # Record the scrape time\n",
    "\n",
    "        for element in post_elements:\n",
    "                \n",
    "                if post_count >= n:\n",
    "                        print(f\"Đã thu thập đủ {n} bài trong nhóm {group_href}, chuyển sang nhóm tiếp theo...\")\n",
    "\n",
    "                        driver.close()\n",
    "\n",
    "                        # Lấy nhóm mới\n",
    "                        group_href = get_group_href()\n",
    "                        \n",
    "                        if not group_href:\n",
    "                            print(\"Không có nhóm nào để tiếp tục, dừng chương trình.\")\n",
    "                            driver.quit()\n",
    "                            exit()\n",
    "                        # Mở tab mới với nhóm mới\n",
    "                        driver.execute_script(f\"window.open('{group_href}');\")\n",
    "                        driver.switch_to.window(driver.window_handles[-1])\n",
    "                        driver.execute_script(\"document.body.style.zoom='60%'\")\n",
    "                        post_count = 0\n",
    "                        continue\n",
    "                scrape_time = time.localtime()  # Record the scrape time\n",
    "                actions.move_to_element(element).perform()\n",
    "                time.sleep(1)  \n",
    "\n",
    "                # Get post href\n",
    "                post_href_element = element.find_element(By.CSS_SELECTOR, \n",
    "                        \"div.html-div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1q0g3np a\")\n",
    "                post_href_value = post_href_element.get_attribute('href')\n",
    "                if post_href_value in post_href_list:\n",
    "                        continue\n",
    "\n",
    "                # Get author name\n",
    "                try:\n",
    "                        author_name_element = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "                        author_name_value = author_name_element[0].text.strip()\n",
    "                except:\n",
    "                        author_name_value = None\n",
    "                # Switch tabs and get HR ID\n",
    "                actions.move_to_element(author_name_element[0]).key_down(Keys.CONTROL).click(author_name_element[0]).key_up(Keys.CONTROL).perform()\n",
    "                time.sleep(2)\n",
    "                current_tabs = driver.window_handles\n",
    "\n",
    "                if len(current_tabs) > 2:\n",
    "                        driver.switch_to.window(current_tabs[-1])\n",
    "                        try :\n",
    "                                profile_elements = driver.find_elements(By.CSS_SELECTOR, 'a.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx')\n",
    "                                hr_id_value = profile_elements[6].get_attribute('href')\n",
    "                        except:\n",
    "                                hr_id_value = None\n",
    "                        time.sleep(2)\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(current_tabs[1])\n",
    "                else:\n",
    "                        print(\"Only one tab, no extra tab to close.\")\n",
    "\n",
    "                # Get post date\n",
    "                try:\n",
    "                                hover_elements = element.find_elements(By.CSS_SELECTOR, 'span.html-span.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x1hl2dhg.x16tdsg8.x1vvkbs')\n",
    "                                # time.sleep(5)\n",
    "                                actions.move_to_element(hover_elements[1]).perform()\n",
    "                                time.sleep(5)\n",
    "                                html =driver.page_source\n",
    "                                date_element = driver.find_element(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1nxh6w3.x1sibtaa.xo1l8bm.xzsf02u\")\n",
    "                                post_date_value = date_element.text\n",
    "                                print(post_date_value)\n",
    "                except Exception as e:\n",
    "                                post_date_value = None\n",
    "\n",
    "                # Check for \"see more\" button and click if present\n",
    "                try:\n",
    "                        see_more_button = element.find_element(By.XPATH, \".//div[contains(@class, 'x1i10hfl') and contains(@role, 'button') and contains(text(), 'Xem thêm')]\")\n",
    "                        if see_more_button:\n",
    "                                see_more_button.click()\n",
    "                                time.sleep(1)\n",
    "                except Exception as e:\n",
    "                        print(\"No 'see more' button found\")\n",
    "\n",
    "                # Get the content\n",
    "                try :\n",
    "                        color_content_elements = element.find_elements(By.CSS_SELECTOR, \"div.x6s0dn4.x78zum5.xdt5ytf.x5yr21d.xl56j7k.x10l6tqk.x17qophe.x13vifvy.xh8yej3\")\n",
    "                        content_element_1s = element.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "\n",
    "                        if color_content_elements:  # Kiểm tra nếu tìm thấy phần tử\n",
    "                                color_content = color_content_elements[0]\n",
    "                                content_element_1s = color_content.find_elements(By.CSS_SELECTOR, \"div.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.x1vvkbs\")\n",
    "                                \n",
    "                                if content_element_1s:\n",
    "                                        print('1')\n",
    "                                        content_value = content_element_1s[0].text\n",
    "                                else:\n",
    "                                        content_value = None\n",
    "                        else:\n",
    "                                raise NoSuchElementException\n",
    "                        \n",
    "                except NoSuchElementException:\n",
    "                        time.sleep(1)\n",
    "                        print('2')\n",
    "                        \n",
    "                        content_element_2s = element.find_elements(By.CSS_SELECTOR, \"span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u\")\n",
    "                        content_value = content_element_2s[0].text if content_element_2s else None\n",
    "                        \n",
    "\n",
    "                # Get images\n",
    "                image_elements = element.find_elements(By.TAG_NAME, 'img')\n",
    "                image_urls = [img.get_attribute('src') for img in image_elements if int(img.get_attribute('width')) > 300 and int(img.get_attribute('height')) > 300]\n",
    "\n",
    "                # Save post data and images\n",
    "                post_data = {\n",
    "                        'post_href': post_href_value,\n",
    "                        'author_name': author_name_value,\n",
    "                        'hr_id': hr_id_value,\n",
    "                        'group_id': group_href,\n",
    "                        'scrape_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'post_date': post_date_value,\n",
    "                        'content': content_value,\n",
    "                        'images': image_urls\n",
    "                }\n",
    "                \n",
    "                # save_post_data(post_data, element.get_attribute('outerHTML'), image_urls, scrape_time)\n",
    "                # save_post_data(post_data,html, image_urls, scrape_time)\n",
    "\n",
    "                # Append post_href_value to the CSV file\n",
    "                save_post_href_to_csv(post_href_value)\n",
    "                # Also append to the list for any further processing in the same run\n",
    "                post_href_list.append(post_href_value)\n",
    "                print(f\"đang thao tác ở post {post_count}\")\n",
    "                post_count += 1\n",
    "\n",
    "                # Sleep before the next post to avoid rate limiting\n",
    "                time.sleep(1)\n",
    "                \n",
    "\n",
    "        # Scroll down the page to load more posts\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
